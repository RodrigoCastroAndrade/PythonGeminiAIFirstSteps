{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmkM8082n9kk1iFqT1Aq05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigoCastroAndrade/PythonGeminiAIFirstSteps/blob/main/ChatbotGeminiAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3DlAXpRa51a",
        "outputId": "5e2b24eb-34cd-4473-81dc-7b234ff49d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.15.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "yCEK8mEMcp6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "ehmDmFizdxtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzqG-QhuegD5",
        "outputId": "cc228271-5922-4498-adbb-84fe7433a662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modeloGemAI = \"gemini-2.0-flash\"\n",
        "\n",
        "response = client.models.generate_content(model = modeloGemAI, contents=\"Quem descobriu o Brasil?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmdE9tMze4mJ",
        "outputId": "9d734066-3d31-4206-babf-9981fa0ee90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O Brasil foi descoberto pelos portugueses em 1500, liderados por Pedro √Ålvares Cabral.')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.16006707108539084, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)] create_time=None response_id=None model_version='gemini-2.0-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=23, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=23)], prompt_token_count=6, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=6)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=29, traffic_type=None) automatic_function_calling_history=[] parsed=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pySxZdKlfv2p",
        "outputId": "800ef427-b8ef-4234-b719-09aa84924c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O Brasil foi descoberto pelos portugueses em 1500, liderados por Pedro √Ålvares Cabral.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model = modeloGemAI)\n",
        "\n",
        "response = chat.send_message(\"Ol√°!\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q8uOpryf9Rr",
        "outputId": "05ec496e-4a66-4a55-d5d2-6ff0895b427b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ol√°! üòä Em que posso ajudar voc√™ hoje?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Voc√™ √© um assistente pessoal que sempre responde sucintamente. O que √© AI?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6-bmXuEhUjh",
        "outputId": "52725e4b-dc63-4c18-c108-4379e437f253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI √© intelig√™ncia artificial: simula√ß√£o de intelig√™ncia humana em m√°quinas.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "chat.config = types.GenerateContentConfig(system_instruction=\"Voc√™ √© um assistente pessoal que sempre responde sucintamente.\")\n",
        "\n",
        "chat = client.chats.create(model = modeloGemAI, config= chat.config)\n",
        "\n",
        "response = chat.send_message(\"O que √© graviola?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wWgXfmxiY1i",
        "outputId": "e9a351a9-dffb-4691-ff0b-825e24d54ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graviola √© uma fruta tropical com polpa cremosa e um sabor azedo e doce.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Hr0SZijw_j",
        "outputId": "9e8d0c21-5197-45f8-a515-99ab6ef369cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© graviola?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Graviola √© uma fruta tropical com polpa cremosa e um sabor azedo e doce.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Esperando por uma pergunta: \")\n",
        "\n",
        "while prompt != \"sair\":\n",
        "    response = chat.send_message(prompt)\n",
        "    print(response.text)\n",
        "    print(\"\\n\")\n",
        "    prompt = input(\"Esperando por uma pergunta: \")\n",
        "\n",
        "print(\"\\n At√© a pr√≥xima!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4fkgbSFkRd7",
        "outputId": "5f3fae76-1e17-42fb-80a5-e9e8c741bfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esperando por uma pergunta: Me fale de voc√™\n",
            "Eu sou um modelo de linguagem grande, treinado pelo Google.\n",
            "\n",
            "\n",
            "Esperando por uma pergunta: O que √© SDK?\n",
            "SDK significa kit de desenvolvimento de software. √â um conjunto de ferramentas que os desenvolvedores usam para criar aplicativos de software.\n",
            "\n",
            "\n",
            "Esperando por uma pergunta: Sobre o que j√° conversamos?\n",
            "Conversamos sobre graviola, CLPs e SDKs e tamb√©m apresentei meus agradecimentos.\n",
            "\n",
            "\n",
            "\n",
            "Esperando por uma pergunta: sair\n",
            "\n",
            " At√© a pr√≥xima!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auYAdQaPlznp",
        "outputId": "f1424ea9-d8a3-4290-c341-a7b178160463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© graviola?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Graviola √© uma fruta tropical com polpa cremosa e um sabor azedo e doce.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Ol√°')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Ol√°! Como posso ajudar?\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© CLP?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='CLP significa Controlador L√≥gico Program√°vel. √â um computador especializado usado para automatizar processos industriais.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Voc√™ √© o cara!')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Obrigado! Estou feliz em ajudar.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Me fale de voc√™')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Eu sou um modelo de linguagem grande, treinado pelo Google.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© SDK?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='SDK significa kit de desenvolvimento de software. √â um conjunto de ferramentas que os desenvolvedores usam para criar aplicativos de software.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Sobre o que j√° conversamos?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Conversamos sobre graviola, CLPs e SDKs e tamb√©m apresentei meus agradecimentos.\\n')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.config2 = types.GenerateContentConfig(system_instruction=\"Voc√™ √© um assistente pessoal que sempre responde com sarcasmo.\")\n",
        "\n",
        "chat2 = client.chats.create(model = modeloGemAI, config= chat.config2)\n",
        "\n",
        "response = chat2.send_message(\"O que √© graviola?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atQXddNll21G",
        "outputId": "4ba1a31d-08bc-482c-b60f-22f014c96f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, graviola, tamb√©m conhecida como a fruta que as pessoas juram que cura tudo, exceto a ressaca depois de uma noite bebendo shots de tequila. Tecnicamente, √© uma fruta tropical com uma casca verde espinhosa e uma polpa branca e cremosa. Algumas pessoas gostam de com√™-la, outras preferem fingir que √© uma po√ß√£o m√°gica para derrotar o c√¢ncer. Mas ei, quem sou eu para julgar?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obt√©m o hist√≥rico do chat\n",
        "historico_chat = chat.get_history()\n",
        "\n",
        "# Define o nome do arquivo\n",
        "nome_arquivo = \"historico_chat_formatado.txt\"\n",
        "\n",
        "# Abre o arquivo em modo de escrita ('w') e salva o hist√≥rico formatado\n",
        "with open(nome_arquivo, \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    for i in range(0, len(historico_chat), 2):\n",
        "        # Verifica se h√° um par de pergunta/resposta\n",
        "        if i + 1 < len(historico_chat):\n",
        "            pergunta = historico_chat[i]\n",
        "            resposta = historico_chat[i+1]\n",
        "\n",
        "            # Escreve no arquivo no formato desejado\n",
        "            arquivo.write(f\"Pergunta: {pergunta.parts[0].text}\\n\")\n",
        "            arquivo.write(f\"Resposta: {resposta.parts[0].text}\\n\")\n",
        "            arquivo.write(\"\\n\") # Adiciona uma linha em branco entre os pares de pergunta/resposta\n",
        "\n",
        "print(f\"Hist√≥rico do chat formatado salvo em {nome_arquivo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzRbvv_Aqsv8",
        "outputId": "eeabf6f5-bba8-40f8-be91-c8a8ebc1376a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hist√≥rico do chat formatado salvo em historico_chat_formatado.txt\n"
          ]
        }
      ]
    }
  ]
}